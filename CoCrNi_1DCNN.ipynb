{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hthPCScXwT28",
        "outputId": "04d69aaf-fc4e-4750-9669-d104418e21dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/CoCrNi_1DCNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iPa5df3wX_A",
        "outputId": "7973200f-13db-45a5-87c2-8a83f020891e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/CoCrNi_1DCNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "import random as rd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "# import datetime"
      ],
      "metadata": {
        "id": "lReAei-DwtGw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "length_size = int(0.69/0.015*2+1)   #1D size\n",
        "training_dir = 'Training/'   #in one folder\n",
        "save_dir = 'saved_models_1CNN_layer_0923/' \n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir) \n",
        "################################################################# \n",
        "#Functions\n",
        "################################################################\n",
        "# Data normalization from 0 to 1 for double column dataframe, returns single column array\n",
        "def normdata(data):\n",
        "    \n",
        "    (len1,w1) = np.shape(data)\n",
        "    ndata = np.zeros([len1,w1//2])\n",
        "    for i in range(w1//2):\n",
        "        ndata[:,i]=(data[:,2*i+1]-min(data[:,2*i+1]))/(max(data[:,2*i+1])-min(data[:,2*i+1]))\n",
        "    return ndata\n",
        "# defien model name\n",
        "def get_model_name(k):\n",
        "    return 'model_'+str(k)+'.h5'\n",
        "# Data augmendatation for simulated XRD patterns\n",
        "def augdata(data,num):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    newaugd=np.zeros([len1,num])\n",
        "    # pard = []\n",
        "    \n",
        "    for i in range(num):\n",
        "        # pard.append (par1[i])\n",
        "        #adding shift\n",
        "        cut = np.random.randint(-2,2)\n",
        "        #shift to left\n",
        "        if cut>=0:\n",
        "            newaugd[:,i] = np.concatenate((data[cut:93,i],np.zeros([cut,])), axis=0)\n",
        "        #shift to right\n",
        "        else:\n",
        "            newaugd[:,i] = np.concatenate((np.zeros([cut*-1,]), data[0:93+cut,i]), axis=0)\n",
        "       \n",
        "        # #plot data augmented curve\n",
        "        # fig2D = plt.figure(figsize=(4,4))                \n",
        "        # ax2D = fig2D.add_subplot(111) \n",
        "        # plt.plot(sim_matrix_FeFe[:93, 0], data[:93, i],  label='Original data') \n",
        "        # plt.plot(sim_matrix_FeFe[:93, 0], newaugd[:93, i], label='Data augment')     \n",
        "        # plt.legend(loc='upper right')\n",
        "        \n",
        "        # fig2D = plt.figure(figsize=(4,4))                \n",
        "        # ax2D = fig2D.add_subplot(111) \n",
        "        # plt.plot(sim_matrix_FeFe[:93, 0], data[93:186, i],  label='Original data') \n",
        "        # plt.plot(sim_matrix_FeFe[:93, 0], newaugd[93:186, i], label='Data augment') \n",
        "        # plt.le\n",
        "        \n",
        "    return newaugd\n",
        "#"
      ],
      "metadata": {
        "id": "MNysqSL9xKeI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Load data and preprocess\n",
        "################################################################\n",
        "\n",
        "sim_matrix = np.load(training_dir+'zSDM_simu_CoCrNi_FCC_CoCo.npy')\n",
        "# Data normalization\n",
        "nsim_matrix = normdata(sim_matrix)\n",
        "\n",
        "sim_L12Co = np.load(training_dir+'zSDM_simu_CoCrNi_L12Co_CoCo.npy')\n",
        "# Data normalization\n",
        "nsim_L12Co = normdata(sim_L12Co)\n",
        "\n",
        "\n",
        "# sim_L12Cr_CoCo = np.load(training_dir+'zSDM_simu_CoCrNi_L12Cr_CoCo.npy')\n",
        "# sim_L12Cr_CrCr = np.load(training_dir+'zSDM_simu_CoCrNi_L12Cr_CrCr.npy')\n",
        "# sim_L12Cr_NiNi = np.load(training_dir+'zSDM_simu_CoCrNi_L12Cr_NiNi.npy')\n",
        "# sim_L12Cr = np.concatenate((sim_L12Cr_CoCo, sim_L12Cr_CrCr, sim_L12Cr_NiNi), axis=0)\n",
        "# # Data normalization\n",
        "# nsim_L12Cr_CoCo = normdata(sim_L12Cr_CoCo)\n",
        "# nsim_L12Cr_CrCr = normdata(sim_L12Cr_CrCr)\n",
        "# nsim_L12Cr_NiNi = normdata(sim_L12Cr_NiNi)\n",
        "# nsim_L12Cr = np.concatenate((nsim_L12Cr_CoCo, nsim_L12Cr_CrCr, nsim_L12Cr_NiNi), axis=0)\n",
        "\n",
        "# sim_L12Ni_CoCo = np.load(training_dir+'zSDM_simu_CoCrNi_L12Ni_CoCo.npy')\n",
        "# sim_L12Ni_CrCr = np.load(training_dir+'zSDM_simu_CoCrNi_L12Ni_CrCr.npy')\n",
        "# sim_L12Ni_NiNi = np.load(training_dir+'zSDM_simu_CoCrNi_L12Ni_NiNi.npy')\n",
        "# sim_L12Ni = np.concatenate((sim_L12Ni_CoCo, sim_L12Ni_CrCr, sim_L12Ni_NiNi), axis=0)\n",
        "# # Data normalization\n",
        "# nsim_L12Ni_CoCo = normdata(sim_L12Ni_CoCo)\n",
        "# nsim_L12Ni_CrCr = normdata(sim_L12Ni_CrCr)\n",
        "# nsim_L12Ni_NiNi = normdata(sim_L12Ni_NiNi)\n",
        "# nsim_L12Ni = np.concatenate((nsim_L12Ni_CoCo, nsim_L12Ni_CrCr, nsim_L12Ni_NiNi), axis=0)\n",
        "\n",
        "# exp_matrix_FeFe = np.load(training_dir+'zSDM_exp_FeAl_BCC_FeFe.npy')\n",
        "# exp_matrix_AlAl = np.load(training_dir+'zSDM_exp_FeAl_BCC_AlAl.npy')\n",
        "# exp_matrix = np.concatenate((exp_matrix_FeFe, exp_matrix_AlAl), axis=0)\n",
        "\n",
        "#%% Labels\n",
        "label_nsim_matrix = np.zeros((nsim_matrix.shape[1], 1), dtype=int)\n",
        "label_nsim_L12Co = np.ones((nsim_L12Co.shape[1], 1), dtype=int)\n",
        "# label_nsim_L12Cr = np.ones((nsim_L12Cr.shape[1], 1), dtype=int)+1\n",
        "# label_nsim_L12Ni = np.ones((nsim_L12Ni.shape[1], 1), dtype=int)+2\n",
        "\n",
        "# # Plot loaded data\n",
        "# num = 1448\n",
        "# fig2D = plt.figure(figsize=(4,4))                \n",
        "# ax2D = fig2D.add_subplot(111)   \n",
        "# plt.plot(sim_matrix_FeFe[:93, 0], sim_matrix[:93 ,num*2+1])\n",
        "\n",
        "# fig2D = plt.figure(figsize=(4,4))                \n",
        "# ax2D = fig2D.add_subplot(111)   \n",
        "# plt.plot(sim_matrix_FeFe[:93, 0], nsim_matrix[:93 ,num])\n",
        "\n",
        "x_sim_ori = np.concatenate((nsim_matrix, nsim_L12Co), axis=1)\n",
        "y_sim_ori = np.concatenate((label_nsim_matrix, label_nsim_L12Co), axis=0)\n",
        "\n",
        "#%%Data augment\n",
        "(len1, sim_num) = np.shape(x_sim_ori)\n",
        "x_sim_augd= augdata(x_sim_ori, sim_num)   \n",
        "y_sim_augd = y_sim_ori\n",
        "# Prepare simulated arrays for training and testing\n",
        "x_sim = np.concatenate((x_sim_ori, x_sim_augd), axis=1).T\n",
        "y_sim = np.concatenate((y_sim_ori, y_sim_augd), axis=0)"
      ],
      "metadata": {
        "id": "yzPbXLXzxNG4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# Perform training and cross-validation\n",
        "################################################################\n",
        "\n",
        "fold = 5 # Number of k-folds\n",
        "\n",
        "k_fold = KFold(n_splits=fold, shuffle=True)\n",
        "\n",
        "# Create auxiliary arrays\n",
        "accuracy=[]\n",
        "logs=[]\n",
        "ground_truth=[]\n",
        "predictions_ord=[]\n",
        "trains=[]\n",
        "tests=[]\n",
        "trains_combine=[]\n",
        "trains_y=[]\n",
        "\n",
        "# split into train, test, and another test dataset\n",
        "all_sim = np.concatenate((x_sim, y_sim), axis=1)\n",
        "#Data random sequence\n",
        "idx = rd.sample(range(all_sim.shape[0]), all_sim.shape[0]) \n",
        "all_sim_shuttle = all_sim[idx]\n",
        "train_test = all_sim_shuttle[:int(all_sim.shape[0]*0.9), :]\n",
        "test_sec = all_sim_shuttle[int(all_sim.shape[0]*0.9):, :]\n",
        "# np.save('test_sec', test_sec)\n",
        "     \n",
        "# Run cross validation and define a-CNN each time in loop\n",
        "for k, (train, test) in enumerate(k_fold.split(train_test[:, :-1], train_test[:, -1])):\n",
        "    print('k=', k)\n",
        "    #Save splits for later use\n",
        "    trains.append(train)\n",
        "    tests.append(test)\n",
        "    # train_x = x_sim[train]\n",
        "    # train_y = y_sim[train]\n",
        "    # test_x = x_sim[test]\n",
        "    # test_y = y_sim[test]\n",
        "    train_x = train_test[train, :-1]\n",
        "    train_y = train_test[train, -1]\n",
        "    test_x = train_test[test, :-1]\n",
        "    test_y = train_test[test, -1]\n",
        "\n",
        "    # Network Parameters\n",
        "    BATCH_SIZE=32\n",
        "    epochs=30\n",
        "    n_input = 93 # \n",
        "    n_features= 1  # CoCo\n",
        "    n_classes = 2 # matrix, L12Co, L12Cr, L12Ni\n",
        "\n",
        "    enc = OneHotEncoder(sparse=False)\n",
        "    \n",
        "    # Define train data\n",
        "    # train_x_dim = train_x.reshape(train_x.shape[0],n_input,n_features)\n",
        "    # train_x_dim = np.stack((train_x[:, :n_input]), axis=0)\n",
        "    train_x_dim = np.reshape(train_x, train_x.shape+(1,))\n",
        "    trains_y.append(train_y)\n",
        "    train_y_hot = enc.fit_transform(train_y.reshape(-1,1))\n",
        "    \n",
        "    # Define test data\n",
        "    # test_x_dim = test_x.reshape(test_x.shape[0],n_input,n_features)\n",
        "    test_x_dim = np.reshape(test_x, test_x.shape+(1,))\n",
        "    test_y_hot = enc.fit_transform(test_y.reshape(-1,1))\n",
        "    \n",
        "    \n",
        "    # Define network structure\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(16, 10, strides=1, padding='same',input_shape=(n_input, n_features), activation='relu', name = \"last_conv\"),  \n",
        "    tf.keras.layers.Dropout(0.5),   \n",
        "    tf.keras.layers.MaxPool1D(pool_size=2, padding='same'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "     \n",
        "    # Compile model\n",
        "    Optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Fit model\n",
        "    history = model.fit(train_x_dim, train_y_hot, batch_size=BATCH_SIZE, epochs=epochs,\n",
        "                      verbose=0, validation_data=(test_x_dim, test_y_hot))\n",
        "\n",
        "    #save model\n",
        "    model_path = os.path.join(save_dir, get_model_name(k))\n",
        "    model.save(model_path)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ipu_xIwxPx4",
        "outputId": "e2d5a656-3848-452f-ddfb-f219f3a3de72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k= 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k= 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k= 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results   \n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "fig = plt.figure() \n",
        "plt.plot(epochs, acc, c='green', linewidth=3, label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, c='orangered', linewidth=3, label='Validation accuracy')\n",
        "plt.xlabel(\"Epochs\", fontsize=15)\n",
        "plt.ylabel(\"Accuracy\", fontsize=15)\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "      \n",
        "fig = plt.figure() \n",
        "plt.plot(epochs, loss, c='green', linewidth=3, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, c='orangered', linewidth=3, label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\", fontsize=15)\n",
        "plt.ylabel(\"Loss\", fontsize=15)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(loc='upper right', fontsize=12) \n",
        "################################################################\n",
        "# Test using simulated data\n",
        "################################################################\n",
        "test_sec_x, test_sec_y = test_sec[:, :-1], test_sec[:, -1]\n",
        "test_sec_y_hot = enc.fit_transform(test_sec_y.reshape(-1,1))\n",
        "# test_sec_x_dim = np.stack((test_sec_x[:, :n_input], test_sec_x[:, n_input:n_input*2], test_sec_x[:, n_input*2:n_input*3]), axis=2)\n",
        "test_sec_x_dim = np.reshape(test_sec_x, test_sec_x.shape+(1,))\n",
        "test_sec_y_predictions = model.predict(test_sec_x_dim[:,:,:]) \n",
        "\n",
        "# # Plot loaded data\n",
        "# row_test = 6\n",
        "# fig2D = plt.figure(figsize=(4,4))                \n",
        "# ax2D = fig2D.add_subplot(111)   \n",
        "# plt.plot(sim_matrix_FeFe[:, 0], test_sec_x_dim[row_test,:,0])\n",
        "\n",
        "# fig2D = plt.figure(figsize=(4,4))                \n",
        "# ax2D = fig2D.add_subplot(111)   \n",
        "# plt.plot(sim_matrix_FeFe[:, 0], test_sec_x_dim[row_test,:,1])\n",
        "\n",
        "#Plotting a Confusion Matrix\n",
        "matrix = confusion_matrix(test_sec_y_hot.argmax(axis=1), test_sec_y_predictions.argmax(axis=1))    \n",
        "cm_plot_labels = ['FCC','CSRO']\n",
        "df_cm = pd.DataFrame(matrix, index = [i for i in cm_plot_labels],\n",
        "                  columns = [i for i in cm_plot_labels])\n",
        "plt.figure()\n",
        "plt.title('Confusion matrix')\n",
        "sn.heatmap(df_cm, cmap=plt.cm.Reds, annot=True, fmt=\"d\")\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n",
        "# break"
      ],
      "metadata": {
        "id": "k5V2hk3s3iZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Grad-CAM algorithm\n",
        "def grad_cam(layer_name, data):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "    last_conv_layer_output, preds = grad_model(data)\n",
        "    #print(preds[0])\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(data)\n",
        "        pred_index = tf.argmax(preds[0])\n",
        "       \n",
        "        class_channel = preds[:, pred_index]\n",
        "        \n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "    \n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0))\n",
        "    \n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    print(np.shape(last_conv_layer_output))\n",
        "    heatmap = last_conv_layer_output * pooled_grads\n",
        "    heatmap = tf.reduce_mean(heatmap, axis=(1))\n",
        "    heatmap = np.expand_dims(heatmap,0)\n",
        "    heatmap = np.maximum(0,heatmap) #relu\n",
        "    return heatmap"
      ],
      "metadata": {
        "id": "1IkRDglcxT0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class activation map from the input layer to the last Conv. layer\n",
        "layer_name = \"last_conv\"\n",
        "label = [\"FCC\", \"Repulsion\"]\n",
        "cnt = 0\n",
        "x_zSDM = np.arange(-0.7, 0.7, 1.4/93).reshape((-1, 1))\n",
        "for i in test_sec_x_dim[:10,:,:]:\n",
        "    data = np.expand_dims(i,0)\n",
        "    #print(np.shape(data))\n",
        "    pred = model.predict(data)[0][0]\n",
        "    print(pred)\n",
        "    if  pred > 0.9:\n",
        "        heatmap = grad_cam(layer_name,data)\n",
        "        print(np.shape(heatmap))\n",
        "        print(f\"Model prediction = Repulsion ({pred}), True label = {label[int(test_sec_y[cnt])]}\")\n",
        "        fig2D = plt.figure(figsize=(8,4))\n",
        "        ax2D = fig2D.add_subplot(111)\n",
        "        plt.imshow(heatmap.transpose(0,1),cmap='Reds', aspect=\"auto\", interpolation='nearest',extent=[-0.7,0.7,i.min(),i.max()], alpha=0.5)\n",
        "        plt.plot(x_zSDM, i[:,0],'forestgreen', lw=2)\n",
        "        #plt.plot(x_zSDM, i[:,1],'deepskyblue', label='Al-Al', lw=2)\n",
        "        plt.colorbar()\n",
        "        #plt.legend(loc=\"lower right\")\n",
        "        plt.tick_params(axis='both', which='major', labelsize=14)\n",
        "        ax2D.set_xlabel('$\\Delta$z, nm', fontsize=14)\n",
        "        ax2D.set_ylabel('Intensity (A.U.)', fontsize=14) \n",
        "        plt.show()\n",
        "        fig2D.savefig('cnt_%.1f.png'%(cnt),dpi=300)\n",
        "\n",
        "    cnt +=1"
      ],
      "metadata": {
        "id": "cfoygQXyxYow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}